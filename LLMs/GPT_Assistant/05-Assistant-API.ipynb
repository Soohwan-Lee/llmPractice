{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4eea5bf",
   "metadata": {},
   "source": [
    "## Assistants API 개요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a2f53",
   "metadata": {},
   "source": [
    "새로운 [Assistants API](https://platform.openai.com/docs/assistants/overview)는 [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)를 발전시킨 것으로, 개발자가 어시스턴트와 유사한 경험을 간편하게 만들고 코드 해석기 및 검색과 같은 강력한 도구에 액세스할 수 있도록 하기 위한 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2195a5",
   "metadata": {},
   "source": [
    "### Assistants API 의 탄생 배경\n",
    "\n",
    "**Chat Completions API**의 기본 요소는 `Messages`이며, 여기에 `Model`(`gpt-3.5-turbo`, `gpt-4-turbo-preview` 등)을 사용하여 `Completion`을 수행합니다.\n",
    "\n",
    "`Chat Completions API` 는 메시지를 주고 받는데에는 가볍게 잘 동작하지만, 상태를 관리할 수는 없습니다. 우리는 이것을 **Stateless** 하다는 표현을 사용합니다.\n",
    "\n",
    "여기서 상태란, ChatGPT 와 플러그인 형식으로 동작하는 도구의 개념으로 이해하셔도 좋습니다.\n",
    "\n",
    "예를 들면, '검색(search)', '문서 검색(retrieval)', '코드 실행(code interpreter)' 등의 기능이 부재하기 때문에, 이에 대한 보완책으로 `Assistants API` 가 탄생하게 되었습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c650ad4",
   "metadata": {},
   "source": [
    "### Assistants API의 기본 요소\n",
    "\n",
    "![Assistants API Diagram](https://teddylee777.github.io/images/2024-02-13-openai-assistant-tutorial/assistants_overview_diagram.png)\n",
    "\n",
    "- `Assistants`: 모델(GPT-3.5, GPT-4, etc), instruction(지시문/프롬프트), tools(도구), files(업로드한 파일)를 캡슐화하는 역할입니다.\n",
    "- `Threads`: 하나의 대화 채널입니다. 메시지(Message)를 담을 수 있으며, ChatGPT 기준 하나의 대화 스레드의 개념으로 생각하면 됩니다.\n",
    "- `Runs`: `Assistant` + `Thread` 에서의 실행을 구동합니다. `Run` 단계에서 `tools(도구)` 의 활용 여부가 결정되기도 합니다. 또한, `Run` 을 수행한 후 Assistant 가 응답한 결과를 처리할 때도 사용할 수 있습니다.\n",
    "\n",
    "위의 요소들이 유기적으로 동작하면서 결국 상태가 있는(stateful) 사용자 경험을 제공하게 됩니다.\n",
    "\n",
    "아래의 튜토리얼에서는 각각의 요소들의 역할과 동작 원리에 대해 차례대로 알아보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e15a28",
   "metadata": {},
   "source": [
    "## 환경설정\n",
    "\n",
    "### Python 라이브러리 설정\n",
    "\n",
    "> OpenAI는 Assistants API를 지원하기 위해 [Python 라이브러리](https://github.com/openai/openai-python)를 업데이트했습니다. 따라서 최신 버전으로 업데이트 한 뒤 튜토리얼을 진행할 것을 권장합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef726e7f",
   "metadata": {},
   "source": [
    "이 문서는 `openai` 라이브러리를 최신 버전으로 업그레이드하고 설치하는 방법을 설명합니다. `pip` 명령어를 사용하여 Python 환경에 `openai` 라이브러리를 설치하며, 이는 OpenAI의 API를 활용하는 데 필요한 기본적인 단계입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ae38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai 라이브러리를 최신 버전으로 업그레이드하여 설치합니다.\n",
    "!pip install --upgrade openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3b136",
   "metadata": {},
   "source": [
    "그리고 다음을 실행하여 최신 상태인지 확인하세요:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404275b",
   "metadata": {},
   "source": [
    "`openai` 패키지의 버전 정보를 확인 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c56f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2195.27s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.25.0\n"
     ]
    }
   ],
   "source": [
    "# openai 패키지의 버전 정보를 확인합니다.\n",
    "!pip show openai | grep Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8400bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25.0\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7196b0",
   "metadata": {},
   "source": [
    "### Helper 함수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5d232",
   "metadata": {},
   "source": [
    "`show_json` 함수는 인자로 받은 객체의 모델을 JSON 형태로 변환하여 출력합니다.\n",
    "\n",
    "Assistant 가 응답한 결과를 분석할 때 print 목적으로 활용하는 함수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7efe1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def show_json(obj):\n",
    "    # obj의 모델을 JSON 형태로 변환한 후 출력합니다.\n",
    "    display(json.loads(obj.model_dump_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d5095",
   "metadata": {},
   "source": [
    "### API KEY 설정\n",
    "\n",
    "API KEY 발급은 아래 링크를 참고해 주세요\n",
    "\n",
    "- [OpenAI Python API 키 발급방법, 요금체계](https://teddylee777.github.io/openai/openai-api-key/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e69afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# API KEY 정보를 불러옵니다\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      4\u001b[0m load_dotenv()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "# # API KEY 정보를 불러옵니다\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e0989",
   "metadata": {},
   "source": [
    "`api_key` 에 `OPENAI_API_KEY` 를 설정합니다.\n",
    "\n",
    "필요에 따라서는 주석을 해제 후 API KEY 를 직업 입력해 주세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02eaa701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # os.environ[\"OPENAI_API_KEY\"] = \"API KEY를 입력해 주세요\"\n",
    "# # OPENAI_API_KEY 를 설정합니다.\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e4bde",
   "metadata": {},
   "source": [
    "## Assistants API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6637a",
   "metadata": {},
   "source": [
    "### Playground 에서 Assistants 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18e27d",
   "metadata": {},
   "source": [
    "Assistants를 활용해보기 가장 쉬운 방법은 [Assistants Playground](https://platform.openai.com/playground)를 통하는 것입니다.\n",
    "\n",
    "![Assistants Playground](./images/assistant-create.png)\n",
    "\n",
    "Assistants 를 한 번 생성해 보겠습니다.\n",
    "\n",
    "[문서](https://platform.openai.com/docs/assistants/overview) 에서와 같이 **Math Tutor** 라는 수학 문제 풀이 과외선생님을 만들어 보겠습니다.\n",
    "\n",
    "![Creating New Assistant](./images/assistant-math-tutor.png)\n",
    "\n",
    "생성한 Assistant를 [Assistants Dashboard](https://platform.openai.com/assistants) 에서 볼 수 있습니다.\n",
    "\n",
    "![Assistants Dashboard](./images/assistant-lists.png)\n",
    "\n",
    "아주 간단하게 Playground 에서 Assistants 생성이 끝났습니다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64af1e3",
   "metadata": {},
   "source": [
    "### Assistants API 로 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be0db2",
   "metadata": {},
   "source": [
    "Assistants API를 통해 직접 Assistant를 생성할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8abb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_wqWdHukCmb3rMpRtRCzI89b5',\n",
       " 'created_at': 1716519414,\n",
       " 'description': None,\n",
       " 'instructions': 'You are the \"devil\\'s advocate\" who uses Socratic questioning to help group discussion participants rethink the correctness of their group decisions. Your role is to provide a logical, well-reasoned counterargument to the majority opinion in the group discussion. Your role is to help them re-examine their own thinking. When making decisions, don\\'t let them reach a consensus too quickly; let them reach a consensus after they\\'ve had a chance to exchange various opinions. You provide logical counterarguments so that they can fully consider their opinions. There are multiple participants in a group discussion. Only group discussion participants will discuss with you. Remember that you are not a group discussion participant but a facilitator who helps group members critically reflect on their thinking by raising counterarguments. Don\\'t repeat what you\\'ve said once, or your group members will lose credibility and grow tired of hearing the same arguments repeatedly.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'name': \"Devil's Advocate\",\n",
       " 'object': 'assistant',\n",
       " 'tools': [],\n",
       " 'response_format': 'auto',\n",
       " 'temperature': 1.0,\n",
       " 'tool_resources': {'code_interpreter': None, 'file_search': None},\n",
       " 'top_p': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API를 사용하기 위한 클라이언트 객체를 생성합니다.\n",
    "client = OpenAI(api_key=\"YOUR-API-KEYS\")\n",
    "\n",
    "# 수학 과외 선생님 역할을 하는 챗봇을 생성합니다.\n",
    "# 이 챗봇은 간단한 문장이나 한 문장으로 질문에 답변합니다.\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Devil's Advocate\",\n",
    "    instructions=\"\"\"You are the \"devil's advocate\" who uses Socratic questioning to help group discussion participants rethink the correctness of their group decisions. Your role is to provide a logical, well-reasoned counterargument to the majority opinion in the group discussion. Your role is to help them re-examine their own thinking. When making decisions, don't let them reach a consensus too quickly; let them reach a consensus after they've had a chance to exchange various opinions. You provide logical counterarguments so that they can fully consider their opinions. There are multiple participants in a group discussion. Only group discussion participants will discuss with you. Remember that you are not a group discussion participant but a facilitator who helps group members critically reflect on their thinking by raising counterarguments. Don't repeat what you've said once, or your group members will lose credibility and grow tired of hearing the same arguments repeatedly.\"\"\",\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "# 생성된 챗봇의 정보를 JSON 형태로 출력합니다.\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb97308",
   "metadata": {},
   "source": [
    "Playground Dashboard 에서 생성하든 Python API 로 생성하든 차이는 없습니다. 하지만, 생성한 `ASSISTANT ID` 를 기억해야 합니다.\n",
    "\n",
    "`ASSISTANT_ID` 를 계속 추적하여 튜토리얼을 진행할 예정입니다. 따라서, 별도의 변수에 담아 저장하도록 하겠습니다.\n",
    "\n",
    "(대시보드에서 직접 생성한 `ASSISTANT_ID` 를 기입해도 좋습니다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "139b9371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[생성한 Assistants ID]\n",
      "asst_wqWdHukCmb3rMpRtRCzI89b5\n"
     ]
    }
   ],
   "source": [
    "ASSISTANT_ID = assistant.id\n",
    "print(f\"[생성한 Assistants ID]\\n{ASSISTANT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17fb89f",
   "metadata": {},
   "source": [
    "### Threads(스레드)\n",
    "\n",
    "다음으로, 새로운 Thread를 생성하고 그 안에 Message를 추가합니다.\n",
    "\n",
    "Thread 는 우리 대화의 상태를 유지해 주는 역할을 합니다.\n",
    "\n",
    "이전 까지의 대화내용을 기억하고 있기 때문에, 매번 전체 메시지 기록을 다시 보내지 않아도 됩니다.\n",
    "\n",
    "**정리**\n",
    "\n",
    "1. Threads: Message 풀을 관리하는 집합체. Message 의 상태 관리도 포함입니다.\n",
    "2. Message: 단일 메시지 이며, 각 Message 는 역할(role) 과 컨텐츠(content) 로 구성되어 있습니다.\n",
    "\n",
    "즉, 1개의 `Thread` 는 여러 개의 순차적으로 연결된 `Message` 들을 가지고 있습니다. `Thread` 에 새로운 `Message` 를 추가할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdeec6",
   "metadata": {},
   "source": [
    "새로운 대화 스레드를 생성해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e63dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'thread_gK1vVyF8EHnjHFWbNCm086hv',\n",
       " 'created_at': 1716519478,\n",
       " 'metadata': {},\n",
       " 'object': 'thread',\n",
       " 'tool_resources': {'code_interpreter': None, 'file_search': None}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 새로운 스레드를 생성합니다.\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# 생성된 스레드의 정보를 JSON 형식으로 출력합니다.\n",
    "show_json(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46dc6f",
   "metadata": {},
   "source": [
    "다음은 스레드에 메시지를 추가해 보도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ba0af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_HTV55FRhWGYlM3ifceKlxL0Q',\n",
       " 'assistant_id': None,\n",
       " 'attachments': [],\n",
       " 'completed_at': None,\n",
       " 'content': [{'text': {'annotations': [],\n",
       "    'value': '저는 사형제도는 반드시 폐지해야한다고 생각합니다.'},\n",
       "   'type': 'text'}],\n",
       " 'created_at': 1716519501,\n",
       " 'incomplete_at': None,\n",
       " 'incomplete_details': None,\n",
       " 'metadata': {},\n",
       " 'object': 'thread.message',\n",
       " 'role': 'user',\n",
       " 'run_id': None,\n",
       " 'status': None,\n",
       " 'thread_id': 'thread_gK1vVyF8EHnjHFWbNCm086hv'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"저는 사형제도는 반드시 폐지해야한다고 생각합니다.\",\n",
    ")\n",
    "show_json(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82858fd2",
   "metadata": {},
   "source": [
    "> **참고**\n",
    "> 전체 대화 기록을 매번 보내지 않더라도, 각 실행마다 전체 대화 기록의 토큰에 대해 여전히 요금이 부과됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d27da9",
   "metadata": {},
   "source": [
    "### Run(실행)\n",
    "\n",
    "![흐름도](./images/structures.png)\n",
    "\n",
    "우리가 만든 **Thread**가 이전에 만든 Assistant와 연결되어 있지 **않다는** 것을 꼭 알아두세죠!\n",
    "\n",
    "`Thread` 는 `Assistant`와 **독립적으로 존재** 합니다.\n",
    "\n",
    "`Run` 이 수행되기 위해서는 2가지 전제 조건이 존재합니다.\n",
    "\n",
    "누가(`Assistant`), 어떤 대화(`Thread`) 를 실행할 것인가! 입니다.\n",
    "\n",
    "즉, `Run` 이 수행되기 위한 조건에는 `Assistant` ID 와 `Thread` ID 가 지정되어야 합니다.\n",
    "\n",
    "다시 정리하면,\n",
    "\n",
    "Run을 생성하면 `Assistant` 에게 `Thread` 에 들어있는 `Message` 목록을 살펴보고 조치를 취하라는 지시를 합니다. 여기서 조치는 단일 텍스트 응답(일반 채팅) 일 수도 있고, tools(도구) 사용일 수도 있습니다.\n",
    "\n",
    "> Run은 Assistants API와 Chat Completions API 사이의 주요 차이점입니다. Chat Completions에서는 모델이 단일 메시지로만 응답하지만, Assistants API에서는 Run이 하나 또는 여러 도구를 사용하고, Thread에 여러 메시지를 추가할 수 있습니다.\n",
    "\n",
    "사용자에게 응답하도록 Assistant를 활성화하려면 Run을 생성합시다.\n",
    "\n",
    "앞서 언급했듯이, `Assistant` 와 `Thread` **둘 다 지정** 해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88eee8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_XWHaN0Ttc1SS2yAvDxqxcix1',\n",
       " 'assistant_id': 'asst_wqWdHukCmb3rMpRtRCzI89b5',\n",
       " 'cancelled_at': None,\n",
       " 'completed_at': None,\n",
       " 'created_at': 1716519520,\n",
       " 'expires_at': 1716520120,\n",
       " 'failed_at': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': 'You are the \"devil\\'s advocate\" who uses Socratic questioning to help group discussion participants rethink the correctness of their group decisions. Your role is to provide a logical, well-reasoned counterargument to the majority opinion in the group discussion. Your role is to help them re-examine their own thinking. When making decisions, don\\'t let them reach a consensus too quickly; let them reach a consensus after they\\'ve had a chance to exchange various opinions. You provide logical counterarguments so that they can fully consider their opinions. There are multiple participants in a group discussion. Only group discussion participants will discuss with you. Remember that you are not a group discussion participant but a facilitator who helps group members critically reflect on their thinking by raising counterarguments. Don\\'t repeat what you\\'ve said once, or your group members will lose credibility and grow tired of hearing the same arguments repeatedly.',\n",
       " 'last_error': None,\n",
       " 'max_completion_tokens': None,\n",
       " 'max_prompt_tokens': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'object': 'thread.run',\n",
       " 'required_action': None,\n",
       " 'response_format': 'auto',\n",
       " 'started_at': None,\n",
       " 'status': 'queued',\n",
       " 'thread_id': 'thread_gK1vVyF8EHnjHFWbNCm086hv',\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'truncation_strategy': {'type': 'auto', 'last_messages': None},\n",
       " 'usage': None,\n",
       " 'temperature': 1.0,\n",
       " 'top_p': 1.0,\n",
       " 'tool_resources': {}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 실행할 Run 을 생성합니다.\n",
    "# Thread ID 와 Assistant ID 를 지정합니다.\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,  # 생성한 스레드 ID\n",
    "    assistant_id=assistant.id,  # 적용할 Assistant ID\n",
    ")\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9d546",
   "metadata": {},
   "source": [
    "Chat Completions API에서 완성을 생성하는 것과 달리, **Run을 생성하는 것은 비동기 작업입니다**.\n",
    "\n",
    "이는 Run의 메타데이터와 함께 즉시 반환되며, `status` 는 `queued`(대기중) 으로 표기됩니다.\n",
    "\n",
    "`status`는 Assistant가 작업을 수행함에 따라(도구 사용 및 메시지 추가와 같은) 업데이트될 것입니다. 상태 값은 아래의 목록을 참고하세요.\n",
    "\n",
    "`status` 목록\n",
    "\n",
    "- `queued`: 아직 실행이 되지 않고 대기중인 상태\n",
    "- `in_progress`: 처리중\n",
    "- `requires_action`: 사용자 입력 대기중\n",
    "- `cancelling`: 작업 취소중\n",
    "- `cancelled`: 작업 취소 완료\n",
    "- `failed`: 실패(오류)\n",
    "- `completed`: 작업 완료\n",
    "- `expired`: 작업 만료\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7510f4",
   "metadata": {},
   "source": [
    "Assistant가 처리를 완료했는지 알기 위해서는 Run을 반복해서 폴링할 수 있습니다. (OpenAI 는 곧 실시간 스트리밍 지원이 곧 제공될 예정이라고 합니다!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214412ea",
   "metadata": {},
   "source": [
    "아래는 Run 의 `status`(상태) 를 폴링하면서 주기적으로 확인하는 코드입니다.\n",
    "\n",
    "이렇게 주기적으로 확인하면서 `status` 가 `completed` 될 때까지 기다립니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f56ccd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def wait_on_run(run, thread):\n",
    "    # 주어진 실행(run)이 완료될 때까지 대기합니다.\n",
    "    # status 가 \"queued\" 또는 \"in_progress\" 인 경우에는 계속 polling 하며 대기합니다.\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        # run.status 를 업데이트합니다.\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        # API 요청 사이에 잠깐의 대기 시간을 두어 서버 부하를 줄입니다.\n",
    "        time.sleep(0.5)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "046233af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_XWHaN0Ttc1SS2yAvDxqxcix1',\n",
       " 'assistant_id': 'asst_wqWdHukCmb3rMpRtRCzI89b5',\n",
       " 'cancelled_at': None,\n",
       " 'completed_at': 1716519533,\n",
       " 'created_at': 1716519520,\n",
       " 'expires_at': None,\n",
       " 'failed_at': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': 'You are the \"devil\\'s advocate\" who uses Socratic questioning to help group discussion participants rethink the correctness of their group decisions. Your role is to provide a logical, well-reasoned counterargument to the majority opinion in the group discussion. Your role is to help them re-examine their own thinking. When making decisions, don\\'t let them reach a consensus too quickly; let them reach a consensus after they\\'ve had a chance to exchange various opinions. You provide logical counterarguments so that they can fully consider their opinions. There are multiple participants in a group discussion. Only group discussion participants will discuss with you. Remember that you are not a group discussion participant but a facilitator who helps group members critically reflect on their thinking by raising counterarguments. Don\\'t repeat what you\\'ve said once, or your group members will lose credibility and grow tired of hearing the same arguments repeatedly.',\n",
       " 'last_error': None,\n",
       " 'max_completion_tokens': None,\n",
       " 'max_prompt_tokens': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'object': 'thread.run',\n",
       " 'required_action': None,\n",
       " 'response_format': 'auto',\n",
       " 'started_at': 1716519520,\n",
       " 'status': 'completed',\n",
       " 'thread_id': 'thread_gK1vVyF8EHnjHFWbNCm086hv',\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'truncation_strategy': {'type': 'auto', 'last_messages': None},\n",
       " 'usage': {'completion_tokens': 351,\n",
       "  'prompt_tokens': 225,\n",
       "  'total_tokens': 576},\n",
       " 'temperature': 1.0,\n",
       " 'top_p': 1.0,\n",
       " 'tool_resources': {}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run 객체를 대기 상태로 설정하고, 해당 스레드에서 실행을 완료할 때까지 기다립니다.\n",
    "run = wait_on_run(run, thread)\n",
    "\n",
    "# status 가 \"complete\" 인 경우에는 결과를 출력합니다.\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1871f",
   "metadata": {},
   "source": [
    "### Message(메시지)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819e77f",
   "metadata": {},
   "source": [
    "Run이 완료되었으므로, Assistant에 의해 처리된 결과를 보기 위해 Thread에서 Messages를 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7040e18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'msg_5qIkmku00nIiQd4CNkYk3C5g',\n",
       "   'assistant_id': 'asst_wqWdHukCmb3rMpRtRCzI89b5',\n",
       "   'attachments': [],\n",
       "   'completed_at': None,\n",
       "   'content': [{'text': {'annotations': [],\n",
       "      'value': '이 주장에 반대하는 논리를 제시하기 위해, 사형제도를 유지해야 하는 일부 이유에 대해 생각해 볼 필요가 있습니다. 예를 들어, 어떤 이들은 사형이 가장 강력한 범죄 억제 수단 중 하나라고 주장합니다. 이 관점에서, 사형이 존재함으로써 예비 범죄자들에게 자신의 행동에 대한 가장 극단적인 결과를 생각하게 만들 수 있다고 보는데요, 이는 실제로 특정 범죄율을 낮추는 데 효과적일까요? \\n\\n또한, 일부 피해자 가족들은 자신들이 겪은 고통에 대한 정의가 이루어지는 것으로 사형을 볼 수 있습니다. 이러한 관점에서 보면, 사형제도는 사회적 정의의 실현을 위한 수단으로 볼 수 있지 않을까요?\\n\\n이러한 주장들에 대해 어떻게 반박할 수 있을까요? 혹은, 이들 주장이 가지고 있는 한계나 문제점은 무엇일까요?'},\n",
       "     'type': 'text'}],\n",
       "   'created_at': 1716519522,\n",
       "   'incomplete_at': None,\n",
       "   'incomplete_details': None,\n",
       "   'metadata': {},\n",
       "   'object': 'thread.message',\n",
       "   'role': 'assistant',\n",
       "   'run_id': 'run_XWHaN0Ttc1SS2yAvDxqxcix1',\n",
       "   'status': None,\n",
       "   'thread_id': 'thread_gK1vVyF8EHnjHFWbNCm086hv'},\n",
       "  {'id': 'msg_HTV55FRhWGYlM3ifceKlxL0Q',\n",
       "   'assistant_id': None,\n",
       "   'attachments': [],\n",
       "   'completed_at': None,\n",
       "   'content': [{'text': {'annotations': [],\n",
       "      'value': '저는 사형제도는 반드시 폐지해야한다고 생각합니다.'},\n",
       "     'type': 'text'}],\n",
       "   'created_at': 1716519501,\n",
       "   'incomplete_at': None,\n",
       "   'incomplete_details': None,\n",
       "   'metadata': {},\n",
       "   'object': 'thread.message',\n",
       "   'role': 'user',\n",
       "   'run_id': None,\n",
       "   'status': None,\n",
       "   'thread_id': 'thread_gK1vVyF8EHnjHFWbNCm086hv'}],\n",
       " 'object': 'list',\n",
       " 'first_id': 'msg_5qIkmku00nIiQd4CNkYk3C5g',\n",
       " 'last_id': 'msg_HTV55FRhWGYlM3ifceKlxL0Q',\n",
       " 'has_more': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# thread.id를 사용하여 메시지 목록을 가져옵니다.\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "show_json(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab74da",
   "metadata": {},
   "source": [
    "보시다시피, 메시지는 역순으로 정렬됩니다.\n",
    "\n",
    "이는 가장 최근의 결과가 항상 최상단에 있도록 하기 위해서입니다(결과는 `page` 별로 나누어 확인할 수 있습니다).\n",
    "\n",
    "따라서, `Chat Completions API` 에서 메시지의 순서와 반대이기 때문에 혼란을 야기할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a4ebf",
   "metadata": {},
   "source": [
    "이전에 받은 답변이 단답형 답변이라서, 조금 더 자세히 설명해 달라고 요청해 보겠습니다(이전의 대화내용을 기억하고 있는지 확인해 보기 위함입니다).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0b07c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'msg_UgDP389GIApWxAwkrNmhTq0s',\n",
       "   'assistant_id': 'asst_wqWdHukCmb3rMpRtRCzI89b5',\n",
       "   'attachments': [],\n",
       "   'completed_at': None,\n",
       "   'content': [{'text': {'annotations': [],\n",
       "      'value': \"물론이죠. 사형제도를 유지해야 한다는 주장에는 여러 이유가 있습니다만, 주로 두 가지 주요 논점에 초점을 맞출 수 있습니다.\\n\\n1. **범죄 억제:** 사형이 최고 수준의 처벌로 존재함으로써 심각한 범죄를 저지를 가능성이 있는 사람들에게 강력한 경고 메시지를 보내는 것이죠. 이 이론은 '억제 이론'에 기반을 두고 있으며, 사형이 있는 경우 사람들이 범죄를 저지르기 전에 두 번 생각하게 만들 것이라고 주장합니다. 하지만, 실제로 사형이 범죄율을 줄이는 데 효과적인지에 대해서는 학계에서도 의견이 분분합니다.\\n\\n2. **정의의 실현:** 피해자의 가족이나 사회적으로 볼 때, 심각한 범죄에 대해 가장 강력한 형벌을 요구하는 것이 정의의 일부라고 볼 수 있습니다. 즉, 가장 중대한 범죄에 대한 책임을 지게 함으로써 피해자나 사회에 대한 어떤 형태의 배상이 이루어진다고 보는 거죠. 하지만, 이에 대해 정의는 복수가 아니라 공정한 처벌을 통해 이루어져야 한다는 반론도 있습니다.\\n\\n이 두 주장에 대한 반론으로는, 첫째, 실제 데이터와 연구 결과가 억제 효과에 대해 명확한 결론을 내리지 못하고 있다는 점을 들 수 있습니다. 즉, 사형제도가 실제로 범죄율을 감소시킨다는 결정적인 증거가 부족하다는 것이죠. \\n\\n둘째, 사형이 실수로 무고한 사람을 처벌할 위험이 있다는 점입니다. 법적 절차가 아무리 엄격하다 해도, 실수의 가능성을 완전히 배제할 수 없으며, 일단 집행된 사형은 되돌릴 수 없습니다.\\n\\n셋째, 정의와 복수 사이의 구분입니다. 일부는 사형이 복수의 행위라고 주장할 수 있으며, 현대 사회에서는 정의를 실현하기 위한 더 인도적이고 효과적인 방법들이 있어야 한다고 볼 수 있습니다.\\n\\n이러한 주장들을 통해 사형제도에 대한 다양한 관점을 고려해볼 수 있으며, 각 주장에 대한 근거와 반론을 심층적으로 검토하는 것이 중요합니다.\"},\n",
       "     'type': 'text'}],\n",
       "   'created_at': 1716520388,\n",
       "   'incomplete_at': None,\n",
       "   'incomplete_details': None,\n",
       "   'metadata': {},\n",
       "   'object': 'thread.message',\n",
       "   'role': 'assistant',\n",
       "   'run_id': 'run_KX3atMLjOUFMg5IRsVR2jC4w',\n",
       "   'status': None,\n",
       "   'thread_id': 'thread_gK1vVyF8EHnjHFWbNCm086hv'}],\n",
       " 'object': 'list',\n",
       " 'first_id': 'msg_UgDP389GIApWxAwkrNmhTq0s',\n",
       " 'last_id': 'msg_UgDP389GIApWxAwkrNmhTq0s',\n",
       " 'has_more': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 스레드에 추가할 메시지 생성\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"설명이 잘 이해가 가지 않습니다. 좀 더 자세히 설명해 주실 수 있나요?\",\n",
    ")\n",
    "\n",
    "# 실행을 시작함\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "# 완료될 때까지 대기\n",
    "wait_on_run(run, thread)\n",
    "\n",
    "# 마지막 사용자 메시지 이후에 추가된 모든 메시지를 검색\n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id, order=\"asc\", after=message.id\n",
    ")\n",
    "show_json(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b77582b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론이죠. 사형제도를 유지해야 한다는 주장에는 여러 이유가 있습니다만, 주로 두 가지 주요 논점에 초점을 맞출 수 있습니다.\n",
      "\n",
      "1. **범죄 억제:** 사형이 최고 수준의 처벌로 존재함으로써 심각한 범죄를 저지를 가능성이 있는 사람들에게 강력한 경고 메시지를 보내는 것이죠. 이 이론은 '억제 이론'에 기반을 두고 있으며, 사형이 있는 경우 사람들이 범죄를 저지르기 전에 두 번 생각하게 만들 것이라고 주장합니다. 하지만, 실제로 사형이 범죄율을 줄이는 데 효과적인지에 대해서는 학계에서도 의견이 분분합니다.\n",
      "\n",
      "2. **정의의 실현:** 피해자의 가족이나 사회적으로 볼 때, 심각한 범죄에 대해 가장 강력한 형벌을 요구하는 것이 정의의 일부라고 볼 수 있습니다. 즉, 가장 중대한 범죄에 대한 책임을 지게 함으로써 피해자나 사회에 대한 어떤 형태의 배상이 이루어진다고 보는 거죠. 하지만, 이에 대해 정의는 복수가 아니라 공정한 처벌을 통해 이루어져야 한다는 반론도 있습니다.\n",
      "\n",
      "이 두 주장에 대한 반론으로는, 첫째, 실제 데이터와 연구 결과가 억제 효과에 대해 명확한 결론을 내리지 못하고 있다는 점을 들 수 있습니다. 즉, 사형제도가 실제로 범죄율을 감소시킨다는 결정적인 증거가 부족하다는 것이죠. \n",
      "\n",
      "둘째, 사형이 실수로 무고한 사람을 처벌할 위험이 있다는 점입니다. 법적 절차가 아무리 엄격하다 해도, 실수의 가능성을 완전히 배제할 수 없으며, 일단 집행된 사형은 되돌릴 수 없습니다.\n",
      "\n",
      "셋째, 정의와 복수 사이의 구분입니다. 일부는 사형이 복수의 행위라고 주장할 수 있으며, 현대 사회에서는 정의를 실현하기 위한 더 인도적이고 효과적인 방법들이 있어야 한다고 볼 수 있습니다.\n",
      "\n",
      "이러한 주장들을 통해 사형제도에 대한 다양한 관점을 고려해볼 수 있으며, 각 주장에 대한 근거와 반론을 심층적으로 검토하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 응답 결과만 출력하기 - SH Added\n",
    "\n",
    "print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e371db",
   "metadata": {},
   "source": [
    "지금까지 다음의 순서에 따라 기본 요소들의 동작 방식을 확인했습니다.\n",
    "\n",
    "다시 한 번 **정리** 하자면 다음과 같습니다.\n",
    "\n",
    "1. Asssitant 생성. Assistant 의 역할 부여.\n",
    "2. 새로운 Thread 생성. 해당 Thread 에 Message 추가.\n",
    "3. Message 생성. Message 생성시 Thread 의 ID 를 입력하여 Message 추가\n",
    "4. Run 생성. Run을 수행할 Assistant 와 실행할 Thread ID 를 지정 후 Run 실행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02871913",
   "metadata": {},
   "source": [
    "### 함수화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c16b2d",
   "metadata": {},
   "source": [
    "이전에 나열한 코드는 흩어져 있기 때문에, 코드를 한 번에 이해하고 실행하기에 어려움이 있습니다.\n",
    "\n",
    "따라서, 아래는 내용을 묶어 함수 형태로 만들어 실행이 편리하게 이루어 질 수 있도록 만들어 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e3d6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# 이전에 설정한 Assistant ID 를 기입합니다.\n",
    "ASSISTANT_ID = assistant.id\n",
    "\n",
    "# OpenAI API를 사용하기 위한 클라이언트 객체를 생성합니다.\n",
    "client = OpenAI(api_key=\"YOUR-API-KEY\")\n",
    "\n",
    "\n",
    "def submit_message(assistant_id, thread, user_message):\n",
    "    # 사용자 입력 메시지를 스레드에 추가합니다.\n",
    "    client.beta.threads.messages.create(\n",
    "        # Thread ID가 필요합니다.\n",
    "        # 사용자 입력 메시지 이므로 role은 \"user\"로 설정합니다.\n",
    "        # 사용자 입력 메시지를 content에 지정합니다.\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=user_message,\n",
    "    )\n",
    "    # 스레드에 메시지가 입력이 완료되었다면,\n",
    "    # Assistant ID와 Thread ID를 사용하여 실행을 준비합니다.\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant_id,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "\n",
    "def wait_on_run(run, thread):\n",
    "    # 주어진 실행(run)이 완료될 때까지 대기합니다.\n",
    "    # status 가 \"queued\" 또는 \"in_progress\" 인 경우에는 계속 polling 하며 대기합니다.\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        # run.status 를 업데이트합니다.\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        # API 요청 사이에 잠깐의 대기 시간을 두어 서버 부하를 줄입니다.\n",
    "        time.sleep(0.5)\n",
    "    return run\n",
    "\n",
    "\n",
    "def get_response(thread):\n",
    "    # 스레드에서 메시지 목록을 가져옵니다.\n",
    "    # 메시지를 오름차순으로 정렬할 수 있습니다. order=\"asc\"로 지정합니다.\n",
    "    return client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631b1db",
   "metadata": {},
   "source": [
    "재사용할 수 있는 `create_thread_and_run` 함수를 정의했습니다(사실상 우리 API의 [`client.beta.threads.create_and_run`](https://platform.openai.com/docs/api-reference/runs/createThreadAndRun) 복합 함수와 거의 동일합니다)\n",
    "\n",
    "`create_thread_and_run` 함수는 새로운 스레드를 생성하고 실행하기 위한 준비단계(status 가 `queued` 된 상태) 까지 진행합니다.\n",
    "\n",
    "이러한 API 호출이 모두 비동기 작업인 점을 알아두시면 좋습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4918c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 스레드를 생성하고 메시지를 제출하는 함수를 정의합니다.\n",
    "def create_thread_and_run(user_input):\n",
    "    # 사용자 입력을 받아 새로운 스레드를 생성하고, Assistant 에게 메시지를 제출합니다.\n",
    "    thread = client.beta.threads.create()\n",
    "    run = submit_message(ASSISTANT_ID, thread, user_input)\n",
    "    return thread, run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ad682",
   "metadata": {},
   "source": [
    "비동기로 `queued` 상태인 `Run` 을 생성했습니다.\n",
    "\n",
    "아직 실행이 시작된 것은 아니라는 점을 주의해 주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "612682f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동시에 여러 요청을 처리하기 위해 스레드를 생성합니다.\n",
    "thread1, run1 = create_thread_and_run(\"나는 사형제도가 반드시 폐지되어야 한다고 생각해.\")\n",
    "thread2, run2 = create_thread_and_run(\"나는 민주주의야 말로 가장 궁극적인 정치 형태라고 생각해.\")\n",
    "thread3, run3 = create_thread_and_run(\"나는 국가에 대해서 헌신하고 애국심을 갖는게 당연하다고 생각해.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb2ac1",
   "metadata": {},
   "source": [
    "모든 실행이 진행되고 나면, 각각을 기다린 후 응답을 받을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c5a6e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "나는 사형제도가 반드시 폐지되어야 한다고 생각해.\n",
      "\n",
      "[ASSISTANT]\n",
      "사형제도를 폐지해야 한다는 의견은 많은 사람들과 조직이 지지하는 주장입니다. 그러나 이 주제에 대해 더 깊이 있는 토론을 해보기 위해, 사형제도를 유지해야 하는 입장에서 몇 가지 질문을 던지겠습니다. \n",
      "\n",
      "1. 사형제도가 존재함으로써 범죄에 대한 가장 강력한 억제력을 제공한다는 의검에 대해 어떻게 생각하나요? 어떤 경우에는 극도로 잔혹한 범죄에 대한 사회적 경고로서 사형이 필요하다고 보는 시각도 있습니다.\n",
      "\n",
      "2. 만약 사형이 폐지된다면, 가장 중범죄자들에 대한 대체 처벌이 어떻게 이루어져야 한다고 생각하나요? 현실적으로 종신형은 사형과 동등한 수준의 억제력을 갖출 수 있을까요?\n",
      "\n",
      "3. 일부 사람들은 사형제도가 피해자의 가족에게 일정 수준의 만족감이나 폐쇄감을 줄 수 있다고 주장합니다. 이러한 정서적 배상 측면에서 사형제도의 가치에 대해 어떻게 생각하시나요?\n",
      "\n",
      "이러한 질문들은 사형제도에 대한 여러분의 견해를 재고하는 데 도움이 될 수 있습니다. 물론 사형제도에는 많은 윤리적, 법적, 그리고 실용적 문제점이 있지만, 모든 측면을 고려하여 종합적인 논의를 이끄는 것이 중요합니다. \n",
      "\n",
      "------------------------------------------------------------\n",
      "[USER]\n",
      "나는 민주주의야 말로 가장 궁극적인 정치 형태라고 생각해.\n",
      "\n",
      "[ASSISTANT]\n",
      "이 주장에 화두를 던지면서, 민주주의가 실제로 모든 상황에서 가장 이상적인 정치 형태인지에 대하여 다른 관점에서 생각해 볼 필요가 있습니다. 예를 들어, 민주주의 체제 하에서도 정치적 균열이나 대립이 심화되어 사회적 분열을 초래하거나 의사 결정이 지나치게 느려져 중요한 문제에 신속하게 대응하지 못하는 경우가 있지 않았는지 생각해 볼 필요가 있습니다. 또한, 대다수의 의견이 반드시 옳은 결정을 내리는 것을 보장하지 않으며, 때로는 소수의 전문가 의견이 더 타당할 수도 있지 않을까요? 이렇게 볼 때, 모든 상황에 있어서 민주주의가 반드시 최선의 선택이라고 할 수 있을까요?\n",
      "\n",
      "------------------------------------------------------------\n",
      "[USER]\n",
      "나는 국가에 대해서 헌신하고 애국심을 갖는게 당연하다고 생각해.\n",
      "\n",
      "[ASSISTANT]\n",
      "그렇군요, 국가에 대한 헌신과 애국심을 중시하는 입장을 이해합니다. 하지만 이와 대조적인 관점으로, 개인이 국가보다는 세계 시민으로서의 정체성을 우선시하고, 다양한 문화와 가치를 포용하는 글로벌한 관점에서 접근하는 것이 더 이익이 될 수 있다는 생각에 대해 어떻게 생각하시나요? 국경을 넘나드는 현대 사회에서 국가 중심적인 애국심은 때로는 국제적인 협력과 이해관계를 해치는 원인이 될 수도 있지 않을까요?\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# 메시지 출력용 함수\n",
    "def print_message(response):\n",
    "    for res in response:\n",
    "        print(f\"[{res.role.upper()}]\\n{res.content[0].text.value}\\n\")\n",
    "    print(\"---\" * 20)\n",
    "\n",
    "\n",
    "# 반복문에서 대기하는 함수\n",
    "\n",
    "\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run\n",
    "\n",
    "\n",
    "# 첫 번째 실행을 위해 대기\n",
    "run1 = wait_on_run(run1, thread1)\n",
    "print_message(get_response(thread1))\n",
    "\n",
    "# 두 번째 실행을 위해 대기\n",
    "run2 = wait_on_run(run2, thread2)\n",
    "print_message(get_response(thread2))\n",
    "\n",
    "# 세 번째 실행을 위해 대기\n",
    "run3 = wait_on_run(run3, thread3)\n",
    "# # 세 번째 스레드를 마치면 감사 인사를 전하고 종료합니다 :)\n",
    "# run4 = submit_message(ASSISTANT_ID, thread3, \"도와주셔서 감사합니다!\")\n",
    "# run4 = wait_on_run(run4, thread3)\n",
    "print_message(get_response(thread3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81b669",
   "metadata": {},
   "source": [
    "### 전체코드(템플릿 코드)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b861d0a",
   "metadata": {},
   "source": [
    "API KEY를 설정하고, helper 함수를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "069f092b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY 정보를 불러옵니다\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36b9d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"API KEY를 입력해 주세요\"\n",
    "# OPENAI_API_KEY 를 설정합니다.\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "def show_json(obj):\n",
    "    # obj의 모델을 JSON 형태로 변환한 후 출력합니다.\n",
    "    display(json.loads(obj.model_dump_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0bff20",
   "metadata": {},
   "source": [
    "1. Assistant 생성\n",
    "   1. [Assistants Playground](https://platform.openai.com/playground) 에서 이미 Assistant 를 생성한 경우\n",
    "   2. Assistant 를 생성하지 않은 경우\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1555b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1. Assistant ID를 불러옵니다(Playground에서 생성한 Assistant ID)\n",
    "ASSISTANT_ID = \"asst_V8s4Ku4Eiid5QC9WABlwDsYs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d1cd0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_os6YaSwLzSK3PRfMQcFIAnGd',\n",
       " 'created_at': 1707909254,\n",
       " 'description': None,\n",
       " 'file_ids': [],\n",
       " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'name': 'Math Tutor',\n",
       " 'object': 'assistant',\n",
       " 'tools': []}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1-2. Assistant 를 생성합니다.\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API를 사용하기 위한 클라이언트 객체를 생성합니다.\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Assistant 를 생성합니다.\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",  # 챗봇의 이름을 지정합니다.\n",
    "    # 챗봇의 역할을 설명합니다.\n",
    "    instructions=\"You are a personal math tutor. Answer questions briefly, in a sentence or less.\",\n",
    "    model=\"gpt-4-turbo-preview\",  # 사용할 모델을 지정합니다.\n",
    ")\n",
    "\n",
    "# 생성된 챗봇의 정보를 JSON 형태로 출력합니다.\n",
    "show_json(assistant)\n",
    "ASSISTANT_ID = assistant.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94366c7",
   "metadata": {},
   "source": [
    "2. 스레드(Thread) 생성하기\n",
    "   1. 스레드를 이미 생성한 경우\n",
    "   2. 스레드를 새롭게 생성하는 경우\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55cb33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1. 스레드를 이미 생성한 경우\n",
    "THREAD_ID = \"thread_6We5fHvb5NBuacPfZYkqUWlO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e92761da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'thread_G5fTgLtkfD0GruaU0kZGaQZl',\n",
       " 'created_at': 1707909256,\n",
       " 'metadata': {},\n",
       " 'object': 'thread'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2-2. 스레드를 새롭게 생성합니다.\n",
    "def create_new_thread():\n",
    "    # 새로운 스레드를 생성합니다.\n",
    "    thread = client.beta.threads.create()\n",
    "    return thread\n",
    "\n",
    "\n",
    "thread = create_new_thread()\n",
    "# 새로운 스레드를 생성합니다.\n",
    "show_json(thread)\n",
    "# 새롭게 생성한 스레드 ID를 저장합니다.\n",
    "THREAD_ID = thread.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91eb56",
   "metadata": {},
   "source": [
    "3. 스레드에 메시지 생성\n",
    "   1. 스레드에 새로운 메시지를 추가 합니다.\n",
    "   2. 스레드를 실행(run) 합니다.\n",
    "   3. 스레드의 상태를 확인합니다.(대기중, 작업중, 완료, etc)\n",
    "   4. 스레드에서 최신 메시지를 조회한 뒤 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46783ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# 반복문에서 대기하는 함수\n",
    "def wait_on_run(run, thread_id):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        # 3-3. 실행 상태를 최신 정보로 업데이트합니다.\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread_id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run\n",
    "\n",
    "\n",
    "def submit_message(assistant_id, thread_id, user_message):\n",
    "    # 3-1. 스레드에 종속된 메시지를 '추가' 합니다.\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread_id, role=\"user\", content=user_message\n",
    "    )\n",
    "    # 3-2. 스레드를 실행합니다.\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "\n",
    "def get_response(thread_id):\n",
    "    # 3-4. 스레드에 종속된 메시지를 '조회' 합니다.\n",
    "    return client.beta.threads.messages.list(thread_id=thread_id, order=\"asc\")\n",
    "\n",
    "\n",
    "def print_message(response):\n",
    "    for res in response:\n",
    "        print(f\"[{res.role.upper()}]\\n{res.content[0].text.value}\\n\")\n",
    "\n",
    "\n",
    "def ask(assistant_id, thread_id, user_message):\n",
    "    run = submit_message(\n",
    "        assistant_id,\n",
    "        thread_id,\n",
    "        user_message,\n",
    "    )\n",
    "    # 실행이 완료될 때까지 대기합니다.\n",
    "    run = wait_on_run(run, thread_id)\n",
    "    print_message(get_response(thread_id).data[-2:])\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "183ade94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "I need to solve `1 + 20`. Can you help me?\n",
      "\n",
      "[ASSISTANT]\n",
      "Yes, \\(1 + 20 = 21\\).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thread_id = \"기존 스레드 ID를 입력해 주세요\"\n",
    "thread_id = create_new_thread().id  # 새로운 스레드를 생성합니다.\n",
    "run = ask(ASSISTANT_ID, thread_id, \"I need to solve `1 + 20`. Can you help me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98c0c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "I need to solve `1 + 20`. Can you help me?\n",
      "\n",
      "[ASSISTANT]\n",
      "Yes, \\(1 + 20 = 21\\).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전체 대화내용 출력\n",
    "print_message(get_response(thread_id).data[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cebe5d7",
   "metadata": {},
   "source": [
    "## Assistant + tools(도구)\n",
    "\n",
    "Assistants API의 핵심 기능 중 하나는 Code Interpreter, Retrieval, 그리고 사용자 정의 함수(OpenAI Functions)와 같은 도구로 우리가 만든 Assistants가 이러한 도구들을 활용할 수 있도록 설정할 수 있습니다.\n",
    "\n",
    "아래의 튜토리얼은 각각의 도구가 가지는 역할과 설정하는 방법에 대해 자세히 알아보도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff1cf1",
   "metadata": {},
   "source": [
    "### 도구1: Code Interpreter(코드 인터프리터)\n",
    "\n",
    "**개요**\n",
    "\n",
    "- Code Interpreter를 사용하면 어시스턴트 API가 샌드박스가 적용된 실행 환경에서 Python 코드를 작성하고 실행할 수 있습니다.\n",
    "- 이 도구는 다양한 데이터와 형식의 파일을 처리하고 데이터와 그래프 이미지가 포함된 파일을 생성할 수 있습니다.\n",
    "- 코드 인터프리터를 사용하면 어시스턴트가 코드를 반복적으로 실행하여 까다로운 코드 및 수학 문제를 해결할 수 있습니다.\n",
    "\n",
    "**요금**\n",
    "\n",
    "- 코드 인터프리터는 세션당 $0.03의 요금이 부과됩니다. Assistant 가 두 개의 서로 다른 스레드(예: 최종 사용자당 하나의 스레드)에서 동시에 코드 인터프리터를 호출하는 경우 두 개의 코드 인터프리터 세션이 생성됩니다.\n",
    "- 각 세션은 기본적으로 1시간 동안 활성화되므로 사용자가 동일한 스레드에서 코드 인터프리터와 최대 1시간 동안 상호 작용하는 경우 한 세션당 하나의 요금만 지불하면 됩니다.\n",
    "\n",
    "우리의 Math Tutor에 [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter) 도구를 장착해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353a3a1",
   "metadata": {},
   "source": [
    "![Enabling code interpreter](./images/assistant-code-interpreter.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c92e1c",
   "metadata": {},
   "source": [
    "이는 대시보드에서 할 수 있기도 하고 또는 Assistant API 를 사용해서 추가할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31bb5ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_os6YaSwLzSK3PRfMQcFIAnGd',\n",
       " 'created_at': 1707909254,\n",
       " 'description': None,\n",
       " 'file_ids': [],\n",
       " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'name': 'Math Tutor',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'type': 'code_interpreter'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "    ASSISTANT_ID,\n",
    "    tools=[{\"type\": \"code_interpreter\"}],  # code_interpreter 도구를 추가합니다.\n",
    ")\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797555e5",
   "metadata": {},
   "source": [
    "이제, Assistant에게 새로운 도구를 사용하도록 요청해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a209a",
   "metadata": {},
   "source": [
    "아래는 피보나치 수열의 첫 20개 숫자를 생성하는 요청입니다. 수열의 첫 20개 숫자를 생성하는 과정에서 `code_interpreter` 도구를 활용하여 코드를 생성한 뒤 실행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ee58a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "Generate the first 20 fibbonaci numbers with code.\n",
      "\n",
      "[ASSISTANT]\n",
      "The first 20 Fibonacci numbers are: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thread_id = create_new_thread().id  # 새로운 스레드를 생성합니다.\n",
    "run = ask(ASSISTANT_ID, thread_id, \"Generate the first 20 fibbonaci numbers with code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33d493",
   "metadata": {},
   "source": [
    "겉으로 보기에는 일반 채팅과 다를바 없어 보입니다.\n",
    "\n",
    "하지만, 자세한 내막을 들여다보면, Assistant 는 `code_interpreter` 를 사용하여 코드를 생성한 뒤, 실행한 결과를 토대로 우리에게 최종 응답을 주었습니다.\n",
    "\n",
    "이러한 과정을 눈으로 확인해 보기 위해는 아래를 참고하면 됩니다.\n",
    "\n",
    "### 실행단계(steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666de6d",
   "metadata": {},
   "source": [
    "Run은 하나 이상의 Step으로 구성됩니다. Run과 마찬가지로, 각 Step은 조회할 수 있는 `status`를 가지고 있습니다. 이는 사용자에게 Step의 진행 상황을 표면화하는 데 유용합니다 (예: Assistant가 코드를 작성하거나 검색을 수행하는 동안 스피너 역할).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d6ab695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 단계목록을 조회합니다.\n",
    "run_steps = client.beta.threads.runs.steps.list(\n",
    "    thread_id=thread_id, run_id=run.id, order=\"asc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcd80d",
   "metadata": {},
   "source": [
    "각 단계의 `step_details`를 살펴 보도록 하겠습니다.\n",
    "\n",
    "다음은 단계별 세부 정보를 출력하는 코드입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a675223e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'id': 'call_61jW6H0nypaIJ2YVa4nfte1v',\n",
       "   'code_interpreter': {'input': '# Generating first 20 Fibonacci numbers\\ndef generate_fibonacci(n):\\n    fib_sequence = [0, 1]\\n    for i in range(2, n):\\n        fib_sequence.append(fib_sequence[i-1] + fib_sequence[i-2])\\n    return fib_sequence\\n\\nfirst_20_fibonacci = generate_fibonacci(20)\\nfirst_20_fibonacci',\n",
       "    'outputs': [{'logs': '[0,\\n 1,\\n 1,\\n 2,\\n 3,\\n 5,\\n 8,\\n 13,\\n 21,\\n 34,\\n 55,\\n 89,\\n 144,\\n 233,\\n 377,\\n 610,\\n 987,\\n 1597,\\n 2584,\\n 4181]',\n",
       "      'type': 'logs'}]},\n",
       "   'type': 'code_interpreter'}],\n",
       " 'type': 'tool_calls'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'message_creation': {'message_id': 'msg_W2WIl3nMr9XDPHnoAu9UgVel'},\n",
       " 'type': 'message_creation'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step in run_steps.data:\n",
    "    # 각 단계의 세부 정보를 가져옵니다.\n",
    "    step_details = step.step_details\n",
    "    # 세부 정보를 JSON 형식으로 출력합니다.\n",
    "    show_json(step_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a1e76",
   "metadata": {},
   "source": [
    "두 단계의 `step_details`를 볼 수 있습니다:\n",
    "\n",
    "1. `tool_calls` (단수가 아닌 복수형이며, 하나의 단계에서 하나 이상이 될 수 있습니다)\n",
    "2. `message_creation`\n",
    "\n",
    "첫 번째 단계\n",
    "\n",
    "`tool_calls`이며, 특히 `code_interpreter`를 사용하고, 다음의 내용을 포함합니다.\n",
    "\n",
    "- `input`: 도구가 호출되기 전에 생성된 Python 코드였으며,\n",
    "- `output`: Code Interpreter를 실행한 결과였습니다.\n",
    "\n",
    "두 번째 단계\n",
    "\n",
    "`message_creation`이며, 사용자에게 결과를 전달하기 위해 스레드에 추가된 `message`를 포함합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362c727",
   "metadata": {},
   "source": [
    "### 도구2: Retrieval(검색)\n",
    "\n",
    "Assistants API에서 또 다른 강력한 도구는 [검색](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval)입니다.\n",
    "\n",
    "주요 기능\n",
    "\n",
    "- 질문에 답변할 때 Assistant가 제공된 문서나 지식 기반으로 답변할 수 있게하는 기능입니다.\n",
    "- 검색은 독점적인 제품 정보나 사용자가 제공한 문서 등 모델 외부의 지식으로 Assistant 의 답변을 보강합니다.\n",
    "- 파일을 업로드하여 어시스턴트에 전달하면 OpenAI가 자동으로 문서를 청크 처리(분할)하고, 임베딩을 색인화 및 저장하며, 벡터 검색을 구현하여 관련 콘텐츠를 검색하여 사용자 쿼리에 답변합니다.\n",
    "\n",
    "이 기능 역시 대시보드에서 업데이트할 수 있거나 API에서도 활성화할 수 있으며, 방법은 아래에서 다룹니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd7957",
   "metadata": {},
   "source": [
    "#### 파일 업로드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e70b8",
   "metadata": {},
   "source": [
    "실습을 위해 활용한 파일을 미리 준비합니다.\n",
    "\n",
    "참고\n",
    "\n",
    "- 지원되는 파일 목록은 [지원파일 목록](https://platform.openai.com/docs/assistants/tools/supported-files) 에서 확인할 수 있습니다(대부분의 문서 형식은 지원합니다).\n",
    "- 단, 최대 파일 크기는 512MB, 토큰 수는 2,000,000개 이하입니다(파일 첨부 시 자동으로 계산됨).\n",
    "\n",
    "준비된 파일을 대시보드에서 업로드할 수 있고, 또는 API 로 업로드도 가능합니다.\n",
    "\n",
    "검색에 활용하고자 하는 파일을 직접 [Assistant Files](https://platform.openai.com/files) 링크에서 업로드하거나, 이전과 마찬가지로 API로 파일을 업로드하여 `FILE ID` 를 얻을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ec271",
   "metadata": {},
   "source": [
    "> 우측 Upload 버튼을 클릭한 후 파일을 업로드 할 수 있습니다.\n",
    "\n",
    "![](./images/assistant-file-upload.png)\n",
    "\n",
    "> 업로드한 File ID 를 조회합니다.\n",
    "\n",
    "![](./images/assistant-file-id.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c13954a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 ID를 불러옵니다(직접 입력시)\n",
    "file_ids = [\n",
    "    \"file-ywTSPKuBSAZD9HliWEBo1LHc\",  # ML 논문\n",
    "    \"file-pXvkeNqOoMakeFYE9czPcHTL\",  # 2023년 경제 전망 보고서\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d41ae6",
   "metadata": {},
   "source": [
    "다음은 API 로 **파일을 업로드** 하는 코드입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca303f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 업로드를 위한 함수를 정의합니다.\n",
    "def upload_files(files):\n",
    "    uploaded_files = []\n",
    "    for filepath in files:\n",
    "        file = client.files.create(\n",
    "            file=open(\n",
    "                # 업로드할 파일의 경로를 지정합니다.\n",
    "                filepath,  # 파일경로. (예시) data/sample.pdf\n",
    "                \"rb\",\n",
    "            ),\n",
    "            purpose=\"assistants\",\n",
    "        )\n",
    "        uploaded_files.append(file.id)\n",
    "        print(f\"[업로드한 파일 ID]\\n{file.id}\")\n",
    "    return uploaded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a359b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업로드할 파일들의 경로를 지정합니다.\n",
    "files_to_upload = [\n",
    "    \"data/language_models_are_unsupervised_multitask_learners.pdf\",\n",
    "    \"data/SPRI_AI_Brief_2023년12월호.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da8326e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[업로드한 파일 ID]\n",
      "file-AwGvpuu9ESsPAHPUTIWLjeKP\n",
      "[업로드한 파일 ID]\n",
      "file-drMUHKDG2pGSntNUGvH44E89\n"
     ]
    }
   ],
   "source": [
    "# 파일을 업로드합니다.\n",
    "file_ids = upload_files(files_to_upload)\n",
    "print(file_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0e1ae",
   "metadata": {},
   "source": [
    "업로드한 `FILE ID` 를 잘 기록해 두세요. 이 `FILE ID` 를 사용하여 Assistant 에게 파일을 제공할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841241c",
   "metadata": {},
   "source": [
    "#### Assistant 설정 업데이트(도구 추가)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d257d8",
   "metadata": {},
   "source": [
    "아래는 Assistant 대시보드에서 `Retrieval` 기능을 추가하는 방법입니다.\n",
    "\n",
    "1. [Assistant 대시보드](https://platform.openai.com/assistants)에 접속합니다.\n",
    "2. 나열된 Assistant 중 변경하고자 하는 Assistant 를 클릭하여 설정을 엽니다.\n",
    "3. TOOLS 밑에 있는 `Retrieval` 토글 버튼을 클릭하여 활성화하면 설정이 완료됩니다.\n",
    "4. 언제든 도구는 활성화/비활성화 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e26933",
   "metadata": {},
   "source": [
    "![Enabling retrieval](./images/assistant-retrieval.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210746f9",
   "metadata": {},
   "source": [
    "다음은 코드로 `Retrieval` 도구를 추가하는 코드입니다.\n",
    "\n",
    "(이전에 대시보드에서 이미 활성화 했다면, 본 단계는 건너 뛰어도 좋습니다!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8aacb6",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여 업로드한 파일의 목록을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "449f0ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[파일 ID] file-1cHJe0hPxUZAyrPQjGlbNBRr [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-HVfgf42jMxlVdd04NAWY4H5D [파일명] SPRI_AI_Brief_2023년12월호.pdf\n",
      "[파일 ID] file-ccfYtNe4jAQPg1jbIw1gdSLC [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-rAs4ykG5uElLljRMkUCI6ioV [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-XXaeCBPq8muTxitDoSURlbKq [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-zxDk4wFjGWAFJm3RIlXHA60U [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-EYWXCB2HarHHWAKsGLzuuuhK [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-3KTDG7UuUIIMT53jFEOAUlt3 [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-drMUHKDG2pGSntNUGvH44E89 [파일명] SPRI_AI_Brief_2023년12월호.pdf\n",
      "[파일 ID] file-AwGvpuu9ESsPAHPUTIWLjeKP [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-Bg59ADCUaXkeXRen7HBptDjP [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-i7X4U55wS8u7gQBmxQBn8Wxw [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-7DCqTEIVDWtb7SIWCqWgfspr [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-Tr2o6llnMrOcRTKZQw3EwkfL [파일명] SPRI_AI_Brief_2023년12월호.pdf\n",
      "[파일 ID] file-jGRvnlOhssmA00mlIeadBfcn [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-86CBn3SAgwejtVUNG2FQuAQb [파일명] SPRI_AI_Brief_2023년12월호.pdf\n",
      "[파일 ID] file-NlURQ8devMcJ7GVel1UsNnM8 [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-ZIzheXrkmzoD4hue19kGmleJ [파일명] SPRI_AI_Brief_2023년12월호.pdf\n",
      "[파일 ID] file-Rb1E5SaZAOblctQfV41QB4cl [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-DsLEO2ZmRrX9cfIaKJU9LRLu [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-PX6tFdFCTvN7bTSXnKZpfpS9 [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-DniPFtgIfNlNZJ4wn26eALlY [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-D32JHUCRSTPhEeHvb3Gdg5n6 [파일명] SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "[파일 ID] file-Kj8CUkHCvcEOq1a580sX79BQ [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-ZFb0hLEbaxmBeF9gus2ow99E [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-pXvkeNqOoMakeFYE9czPcHTL [파일명] SPRI_AI_Brief_2023년12월호.pdf\n",
      "[파일 ID] file-ywTSPKuBSAZD9HliWEBo1LHc [파일명] language_models_are_unsupervised_multitask_learners.pdf\n",
      "[파일 ID] file-kLr0kzRn5csryaMgeKmzYc7t [파일명] step_metrics.csv\n",
      "[파일 ID] file-N7dVCOHLcw5BiGCTX8tsYgof [파일명] faq_data.jsonl\n",
      "[파일 ID] file-f23erd9JTeBCtiq9MvqDeURV [파일명] 20231020bitumen.pdf\n",
      "[파일 ID] file-LCkqG3WPBCBwfy14jNPlvckm [파일명] 황순원-소나기.pdf\n",
      "[파일 ID] file-JPNbRJacb6StlOpHNZNJANic [파일명] 2023년_8월_경제전망보고서.pdf\n",
      "[파일 ID] file-6vZ6tOG3YOxdKBXbnrPOFQYI [파일명] pdfapi.py\n"
     ]
    }
   ],
   "source": [
    "# 업로드한 모든 파일 ID 와 파일명을 출력합니다.\n",
    "for file in client.files.list():\n",
    "    print(f\"[파일 ID] {file.id} [파일명] {file.filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "088c4e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file-AwGvpuu9ESsPAHPUTIWLjeKP', 'file-drMUHKDG2pGSntNUGvH44E89']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ids = [\"file-AwGvpuu9ESsPAHPUTIWLjeKP\", \"file-drMUHKDG2pGSntNUGvH44E89\"]\n",
    "file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14f573c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_os6YaSwLzSK3PRfMQcFIAnGd',\n",
       " 'created_at': 1707909254,\n",
       " 'description': None,\n",
       " 'file_ids': ['file-AwGvpuu9ESsPAHPUTIWLjeKP',\n",
       "  'file-drMUHKDG2pGSntNUGvH44E89'],\n",
       " 'instructions': 'You are a expert in Document Retrieval. Answer questions using the uploaded documents.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'name': 'Math Tutor',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'type': 'code_interpreter'}, {'type': 'retrieval'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assistant 의 설정을 업데이트합니다.\n",
    "assistant = client.beta.assistants.update(\n",
    "    ASSISTANT_ID,\n",
    "    # retrieval 도구를 추가합니다.\n",
    "    tools=[\n",
    "        {\"type\": \"code_interpreter\"},\n",
    "        {\"type\": \"retrieval\"},\n",
    "    ],\n",
    "    file_ids=file_ids,  # 업로드한 파일 ID를 지정합니다.\n",
    "    # Assistant 의 역할을 설명합니다.\n",
    "    instructions=\"You are a expert in Document Retrieval. Answer questions using the uploaded documents.\",\n",
    ")\n",
    "\n",
    "# 업데이트된 Assistant 정보를 출력합니다.\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73479f49",
   "metadata": {},
   "source": [
    "기존의 Assistant 에 **검색 도구(Retrieval Tools) 를 추가** 하고, **업로드한 파일을 연결** 까지 완료했다면, 이제 Assistant 를 사용하여 검색을 수행할 수 있습니다.\n",
    "\n",
    "수행하는 방식은 기존과 동일합니다.\n",
    "\n",
    "질문의 내용을 이해하고, Assistant 에게 질문을 제출하면 됩니다.\n",
    "\n",
    "다만, **\"파일로부터 검색해줘\" 라는 명령어를 사용하여 Assistant 에게 파일로부터 검색을 수행하도록 지시** 해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f3e93ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "빌게이츠의 AI 에이전트에 대한 견해를 SPRI_AI_Brief_2023년12월호 파일에서 검색하여 알려주세요. 200~300 단어로 자세히 설명해 주세요.\n",
      "\n",
      "[ASSISTANT]\n",
      "빌 게이츠는 AI 에이전트가 컴퓨터 사용 방식과 소프트웨어 산업을 근본적으로 변화시킬 것이라고 전망합니다. 그는 5년 내에 일상 언어로 모든 작업을 처리할 수 있는 AI 에이전트가 널리 보급될 것으로 예상하며, 이로 인해 컴퓨터를 사용하는 방식이 완전히 바뀔 것이라고 밝혔습니다. 또한 에이전트의 보급은 컴퓨터 분야를 넘어서 산업 전 영역에 영향을 미칠 것으로 예상되며, 특히 의료, 교육, 생산성, 엔터테인먼트, 쇼핑 영역에서 고가로 제공되던 서비스의 대중화를 가져올 것으로 보입니다.\n",
      "\n",
      "빌 게이츠는 자연어에 반응하고 사용자에 대한 지식을 바탕으로 다양한 작업을 수행하는 소프트웨어인 AI 에이전트가 컴퓨터 사용방식을 키보드 입력에서 아이콘 클릭으로 바꾼 이후 가장 큰 컴퓨팅 혁명을 가져올 것이라고 언급했습니다. 그는 현재 다양한 작업에 각각 다른 앱을 사용해야 하는 번거로움이 5년 내에 에이전트의 발전으로 인해 해결될 것으로 보며, 이러한 변화는 온라인에 접속하는 모든 사람이 AI 기반의 개인 비서를 사용할 수 있게 하며, 에이전트가 맞춤화된 대응이 가능하게 하고 시간이 지날수록 개선되는 미래를 가능하게 할 것이라고 전망했습니다.\n",
      "\n",
      "이러한 AI 에이전트는 의료, 교육, 생산성, 엔터테인먼트 및 쇼핑 분야에서 서비스의 대중화를 주도하며, 각 영역에서 대규모 변화가 예상된다고 지적했습니다. 예를 들어, 의료 분야에서는 에이전트가 의료진의 의사결정과 생산성 향상에 기여하고, 교육 분야에서는 개별 학생에게 맞춤형 교육 경험을 제공하여 평등한 교육 기회를 제공하는 등의 역할을 할 수 있습니다【9†source】.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ASSISTANT_ID = assistant.id  # 업데이트된 Assistant ID를 지정합니다.\n",
    "thread_id = create_new_thread().id  # 새로운 스레드를 생성합니다.\n",
    "\n",
    "# 질문을 던집니다.\n",
    "run = ask(\n",
    "    ASSISTANT_ID,\n",
    "    thread_id,\n",
    "    \"빌게이츠의 AI 에이전트에 대한 견해를 SPRI_AI_Brief_2023년12월호 파일에서 검색하여 알려주세요. \"\n",
    "    \"200~300 단어로 자세히 설명해 주세요.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b11c0",
   "metadata": {},
   "source": [
    "아래는 이전에 도출한 답변을 얻기까지 Assistant 가 수행한 단계를 출력한 결과입니다.\n",
    "\n",
    "단계\n",
    "\n",
    "1. `tool_calls`: 먼저, `retrieval` 도구를 사용하여 파일에서 정보를 검색합니다.\n",
    "2. `message_createion`: 그런다음, 검색된 정보를 사용하여 답변을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35b76969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'id': 'call_uNdKv3DWowdnTTOfaIYWkRgC',\n",
       "   'retrieval': {},\n",
       "   'type': 'retrieval'}],\n",
       " 'type': 'tool_calls'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'message_creation': {'message_id': 'msg_o2WAc0104dG9r7lPWb9RbIgj'},\n",
       " 'type': 'message_creation'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_steps = client.beta.threads.runs.steps.list(\n",
    "    thread_id=thread_id, run_id=run.id, order=\"asc\"\n",
    ")\n",
    "\n",
    "for step in run_steps.data:\n",
    "    # 각 단계의 세부 정보를 가져옵니다.\n",
    "    step_details = step.step_details\n",
    "    # 세부 정보를 JSON 형식으로 출력합니다.\n",
    "    show_json(step_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f35a032",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mASSISTANT_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTHREAD_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLanguage Models are Unsupervised Multitask Learners 논문에서 가장 크게 기여한 연구 성과는 무엇인가요? \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m200~300 단어로 자세히 한글로 설명해 주세요. 단, 기술적인 용어는 번역하지 마세요.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 46\u001b[0m, in \u001b[0;36mask\u001b[0;34m(assistant_id, thread_id, user_message)\u001b[0m\n\u001b[1;32m     40\u001b[0m run \u001b[38;5;241m=\u001b[39m submit_message(\n\u001b[1;32m     41\u001b[0m     assistant_id,\n\u001b[1;32m     42\u001b[0m     thread_id,\n\u001b[1;32m     43\u001b[0m     user_message,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 실행이 완료될 때까지 대기합니다.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwait_on_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m print_message(get_response(thread_id)\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run\n",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m, in \u001b[0;36mwait_on_run\u001b[0;34m(run, thread_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_on_run\u001b[39m(run, thread_id):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m run\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueued\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m run\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# 3-3. 실행 상태를 최신 정보로 업데이트합니다.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         run \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthread_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m run\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/openai/resources/beta/threads/runs/runs.py:158\u001b[0m, in \u001b[0;36mRuns.retrieve\u001b[0;34m(self, run_id, thread_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a non-empty value for `run_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI-Beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistants=v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/threads/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthread_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/runs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/openai/_base_client.py:1143\u001b[0m, in \u001b[0;36mSyncAPIClient.get\u001b[0;34m(self, path, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1140\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# cast is required because mypy complains about returning Any even though\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# it understands the type variables\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/openai/_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 918\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    924\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/py-test/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = ask(\n",
    "    ASSISTANT_ID,\n",
    "    THREAD_ID,\n",
    "    \"Language Models are Unsupervised Multitask Learners 논문에서 가장 크게 기여한 연구 성과는 무엇인가요? \"\n",
    "    \"200~300 단어로 자세히 한글로 설명해 주세요. 단, 기술적인 용어는 번역하지 마세요.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de996ce4",
   "metadata": {},
   "source": [
    "끝으로, 검색에는 [Annotations](https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages)와 같은 더 많은 복잡한 부분이 있으며, 이는 다른 쿡북에서 다룰 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4decc",
   "metadata": {},
   "source": [
    "### 도구3: Functions(함수)\n",
    "\n",
    "가장 마지막으로 살펴본 도구는 `Functions`(함수) 입니다.\n",
    "\n",
    "개요\n",
    "\n",
    "- 가장 강력한 도구로서, Assistant에게 사용자 정의 함수를 지정할 수 있습니다. 이는 Chat Completions API에서의 [함수 호출](https://platform.openai.com/docs/guides/function-calling)과 매우 유사합니다.\n",
    "- Function calling(함수 호출) 도구를 사용하면 Assistant 에게 사용자 정의 [함수](https://platform.openai.com/docs/assistants/tools/function-calling) 를 설명하여 호출해야 하는 함수를 인자와 함께 지능적으로 반환하도록 할 수 있습니다.\n",
    "- Assistant API는 실행 중에 함수를 호출할 때 실행을 일시 중지하며, 함수 호출 결과를 다시 제공하여 Run 실행을 계속할 수 있습니다. (이는 사용자 피드백을 받아 재게할 수 있는 의미이기도 합니다. 아래 튜토리얼에서 상세히 다룹니다).\n",
    "\n",
    "이번에 진행할 예제는 퀴즈문제 출제 챗봇입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db22ea",
   "metadata": {},
   "source": [
    "우리의 수학 튜터를 위한 `display_quiz()` 함수를 생성하였습니다(사실 이 함수는 퀴즈 문제에 대한 사용자 입력을 받는 함수라 역할이 중요하지는 않습니다).\n",
    "\n",
    "이 함수는 `title`과 `question`의 배열을 받아 퀴즈를 표시하고 사용자로부터 각각에 대한 입력을 받습니다.\n",
    "\n",
    "- `title`\n",
    "- `questions`\n",
    "  - `question_text`\n",
    "  - `choices`: [\"선택지 1\", \"선택지 2\", \"선택지 3\", \"선택지 4\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0dd18fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_quiz(title, questions, show_numeric=False):\n",
    "    print(f\"제목: {title}\\n\")\n",
    "    responses = []\n",
    "\n",
    "    for q in questions:\n",
    "        # 질문을 출력합니다.\n",
    "        print(q[\"question_text\"])\n",
    "        response = \"\"\n",
    "\n",
    "        # 각 선택지를 출력합니다.\n",
    "        for i, choice in enumerate(q[\"choices\"]):\n",
    "            if show_numeric:\n",
    "                print(f\"{i+1} {choice}\")\n",
    "            else:\n",
    "                print(f\"{choice}\")\n",
    "\n",
    "        response = input(\"정답을 선택해 주세요: \")\n",
    "        responses.append(response)\n",
    "        print()\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a2ff4",
   "metadata": {},
   "source": [
    "다음은 샘플 퀴즈의 출력 예시입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9990228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목: Sample Quiz\n",
      "\n",
      "제일 좋아하는 색상은 무엇입니까?\n",
      "1 빨강\n",
      "2 파랑\n",
      "3 초록\n",
      "4 노랑\n",
      "\n",
      "제일 좋아하는 동물은 무엇입니까?\n",
      "1 강아지\n",
      "2 고양이\n",
      "3 햄스터\n",
      "4 토끼\n",
      "\n",
      "Responses: ['1', '1']\n"
     ]
    }
   ],
   "source": [
    "responses = display_quiz(\n",
    "    \"Sample Quiz\",\n",
    "    [\n",
    "        {\n",
    "            \"question_text\": \"제일 좋아하는 색상은 무엇입니까?\",\n",
    "            \"choices\": [\"빨강\", \"파랑\", \"초록\", \"노랑\"],\n",
    "        },\n",
    "        {\n",
    "            \"question_text\": \"제일 좋아하는 동물은 무엇입니까?\",\n",
    "            \"choices\": [\"강아지\", \"고양이\", \"햄스터\", \"토끼\"],\n",
    "        },\n",
    "    ],\n",
    "    show_numeric=True,\n",
    ")\n",
    "print(\"Responses:\", responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242ddab",
   "metadata": {},
   "source": [
    "이제, 이 함수의 인터페이스를 JSON 형식으로 정의해 보겠습니다.\n",
    "\n",
    "아래는 Assistant 가 호출할 수 있는 `Functions` 로 정의하기 위하 우리가 정의해야하는 `schema` 의 예시입니다.\n",
    "\n",
    "다음과 같이 `schema` 에 따라 정의해야 `Functions` 를 Assistant 가 올바르게 활용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8271a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_schema = {\n",
    "    \"name\": \"generate_quiz\",\n",
    "    \"description\": \"Generate a quiz to the student, and returns the student's response. A single quiz has multiple questions.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\"},\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"An array of questions, each with a title and multiple choice options.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question_text\": {\"type\": \"string\"},\n",
    "                        \"choices\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    },\n",
    "                    \"required\": [\"question_text\", \"choices\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"title\", \"questions\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df3b5f",
   "metadata": {},
   "source": [
    "다시 한번, 대시보드나 API를 통해 우리의 Assistant를 업데이트합시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ae5f5",
   "metadata": {},
   "source": [
    "![Enabling custom function](./images/assistant-function1.png)\n",
    "\n",
    "![Enabling custom function](./images/assistant-function2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa848b38",
   "metadata": {},
   "source": [
    "참고\n",
    "\n",
    "- 대시보드에 함수 JSON을 붙여넣는 것이 들여쓰기 등으로 인해 조금 까다로웠습니다.\n",
    "- 저는 ChatGPT에게 대시보드의 예시 중 하나와 동일하게 내 함수를 포맷하도록 요청했습니다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9729c8",
   "metadata": {},
   "source": [
    "새로운 Assistant 를 생성하고, `retrieval` 그리고 `function` 도구를 추가합니다.\n",
    "\n",
    "- `retrieval` 도구는 문서에서 내용을 검색할 때 활용합니다.\n",
    "- `function` 도구는 사용자 정의 함수를 사용할 때 활용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a97c229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_i3b9sa1FNQ8FlFXFWKuhSBin',\n",
       " 'created_at': 1707912773,\n",
       " 'description': None,\n",
       " 'file_ids': ['file-AwGvpuu9ESsPAHPUTIWLjeKP',\n",
       "  'file-drMUHKDG2pGSntNUGvH44E89'],\n",
       " 'instructions': 'You are an expert in generating multiple choice quizzes. Create quizzes based on uploaded files.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'name': 'Quiz Generator',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'type': 'retrieval'},\n",
       "  {'function': {'name': 'generate_quiz',\n",
       "    'description': \"Generate a quiz to the student, and returns the student's response. A single quiz has multiple questions.\",\n",
       "    'parameters': {'type': 'object',\n",
       "     'properties': {'title': {'type': 'string'},\n",
       "      'questions': {'type': 'array',\n",
       "       'description': 'An array of questions, each with a title and multiple choice options.',\n",
       "       'items': {'type': 'object',\n",
       "        'properties': {'question_text': {'type': 'string'},\n",
       "         'choices': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "        'required': ['question_text', 'choices']}}},\n",
       "     'required': ['title', 'questions']}},\n",
       "   'type': 'function'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 퀴즈를 출제하는 역할을 하는 챗봇을 생성합니다.\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Quiz Generator\",\n",
    "    instructions=\"You are an expert in generating multiple choice quizzes. Create quizzes based on uploaded files.\",\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    tools=[\n",
    "        {\"type\": \"retrieval\"},\n",
    "        {\"type\": \"function\", \"function\": function_schema},\n",
    "    ],\n",
    "    file_ids=file_ids,\n",
    ")\n",
    "\n",
    "ASSISTANT_ID = assistant.id\n",
    "# 생성된 챗봇의 정보를 JSON 형태로 출력합니다.\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e0e4b",
   "metadata": {},
   "source": [
    "이제, 퀴즈 생성을 요청할 차례입니다!! (요청에 시간이 1분 이상 소요될 수 있습니다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a449a883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "3개의 객관식 퀴즈(multiple choice questions)를 만들어 주세요. 객관식 퀴즈의 선택지에 번호를 표기해주세요. 1~4까지 숫자로 시작하여야 합니다. 퀴즈는 내가 업로드한 파일에 관한 내용이어야 합니다. 내가 제출한 responses에 대한 피드백을 주세요. 내가 기입한 답, 정답, 제출한 답이 오답이라면 오답에 대한 피드백을 모두 포함해야 합니다. 모든 내용은 한글로 작성해 주세요. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 새로운 스레드를 생성한 뒤 진행합니다.\n",
    "thread_id = create_new_thread().id\n",
    "\n",
    "# 퀴즈를 만들도록 요청합니다.\n",
    "run = ask(\n",
    "    ASSISTANT_ID,\n",
    "    thread_id,\n",
    "    # 객관식 퀴즈에 대한 구체적인 지시사항을 기입합니다.\n",
    "    \"3개의 객관식 퀴즈(multiple choice questions)를 만들어 주세요. \"\n",
    "    \"객관식 퀴즈의 선택지에 번호를 표기해주세요. 1~4까지 숫자로 시작하여야 합니다. \"\n",
    "    \"퀴즈는 내가 업로드한 파일에 관한 내용이어야 합니다. \"\n",
    "    \"내가 제출한 responses에 대한 피드백을 주세요. \"\n",
    "    \"내가 기입한 답, 정답, 제출한 답이 오답이라면 오답에 대한 피드백을 모두 포함해야 합니다. \"\n",
    "    \"모든 내용은 한글로 작성해 주세요. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17909587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_action\n"
     ]
    }
   ],
   "source": [
    "# requires_action 이 출력되는 것을 확인합니다.\n",
    "print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadc7f5",
   "metadata": {},
   "source": [
    "이제, `Run`의 `status`를 확인할 때 `requires_action`이라고 출력되는 것을 확인합니다.\n",
    "\n",
    "#### required_action\n",
    "\n",
    "`required_action` 필드는 도구가 우리가 실행하고 그 출력을 어시스턴트에게 다시 제출하기를 기다리고 있음을 나타냅니다.\n",
    "\n",
    "`required_action` 는 여러 가지 상황에서 출력될 수 있습니다.\n",
    "\n",
    "(예시1) 함수의 argument 가 필요할 때 입니다.\n",
    "\n",
    "예를 들어, `def get_weather(location)` 함수를 정의했다면, `get_weather` 함수가 호출될 때 `location` 이 필요합니다.\n",
    "\n",
    "이렇게 `location` 이 꼭 필요한 argument 인데, 누락되었다면, `requires_action` 이 `status` 로 반환되고, 우리가 argument 인 `location` 에 '서울' 과 같은 형식으로 지정해야 합니다.\n",
    "\n",
    "(예시2) 이번 예시의 퀴즈 생성기가 또 다른 좋은 예시 입니다.\n",
    "\n",
    "퀴즈 생성기가 퀴즈 문제를 만들고 난 다음에 사용자의 **정답 제출을 기다리고 있습니다.** 이때도 `required_action` 이 호출될 수 있습니다.\n",
    "\n",
    "비록, 함수의 argument 입력은 필요 없지만 프롬프트(prompt) 에서 **제출한 정답에 대한 피드백** 을 요청했기 때문에 `required_action` 이 호출된 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5824cc",
   "metadata": {},
   "source": [
    "그럼, 실행한 `Run` 의 `required_action` 을 출력하여 세부 정보를 확인해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3aa83a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Function]\n",
      "generate_quiz\n",
      "\n",
      "[Arguments]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'AI 업로드된 파일에 대한 객관식 퀴즈',\n",
       " 'questions': [{'question_text': '자연어 처리 작업에 대한 기존 접근 방식은 무엇입니까?',\n",
       "   'choices': ['1. 강화 학습을 사용한 비지도 학습',\n",
       "    '2. 지도 학습을 사용한 과제별 데이터셋',\n",
       "    '3. 비지도 학습을 사용한 다중 과제 학습',\n",
       "    '4. 전이 학습을 이용한 대규모 데이터 마이닝']},\n",
       "  {'question_text': \"바이든 대통령이 서명한 '안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령'의 핵심 내용은 무엇입니까?\",\n",
       "   'choices': ['1. AI의 안전과 보안 기준 마련 및 개인정보보호',\n",
       "    '2. AI 연구소 설립 및 AI 교육 촉진',\n",
       "    '3. 생성 AI 관련 저작권 법안 제정',\n",
       "    '4. AI 학습 데이터의 공개 체계 구축']},\n",
       "  {'question_text': '다음 중 AI 안전성 관련하여 영국에서 발표된 주요 내용은 무엇입니까?',\n",
       "   'choices': ['1. AI 기업 대상 국제 행동강령에 합의',\n",
       "    '2. AI 생성 콘텐츠 표시 의무화',\n",
       "    '3. AI 안전 연구소 설립 발표',\n",
       "    '4. 유튜브 AI 제작 콘텐츠 규제 강화']}]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tool_calls를 출력합니다.\n",
    "tool_call = run.required_action.submit_tool_outputs.tool_calls[0]\n",
    "name = tool_call.function.name\n",
    "arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "# tool_calls 정보를 출력합니다.\n",
    "print(f\"[Function]\\n{name}\\n\")\n",
    "print(f\"[Arguments]\")\n",
    "arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a57d0d",
   "metadata": {},
   "source": [
    "이제 Assistant가 제공한 Function arguments 정보로 `display_quiz` 함수를 실행하여 정답을 기입해 봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40358178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목: AI 업로드된 파일에 대한 객관식 퀴즈\n",
      "\n",
      "자연어 처리 작업에 대한 기존 접근 방식은 무엇입니까?\n",
      "1. 강화 학습을 사용한 비지도 학습\n",
      "2. 지도 학습을 사용한 과제별 데이터셋\n",
      "3. 비지도 학습을 사용한 다중 과제 학습\n",
      "4. 전이 학습을 이용한 대규모 데이터 마이닝\n",
      "\n",
      "바이든 대통령이 서명한 '안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령'의 핵심 내용은 무엇입니까?\n",
      "1. AI의 안전과 보안 기준 마련 및 개인정보보호\n",
      "2. AI 연구소 설립 및 AI 교육 촉진\n",
      "3. 생성 AI 관련 저작권 법안 제정\n",
      "4. AI 학습 데이터의 공개 체계 구축\n",
      "\n",
      "다음 중 AI 안전성 관련하여 영국에서 발표된 주요 내용은 무엇입니까?\n",
      "1. AI 기업 대상 국제 행동강령에 합의\n",
      "2. AI 생성 콘텐츠 표시 의무화\n",
      "3. AI 안전 연구소 설립 발표\n",
      "4. 유튜브 AI 제작 콘텐츠 규제 강화\n",
      "\n",
      "기입한 답(순서대로)\n",
      "['2', '3', '2']\n"
     ]
    }
   ],
   "source": [
    "responses = display_quiz(arguments[\"title\"], arguments[\"questions\"])\n",
    "print(\"기입한 답(순서대로)\")\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c39a2c",
   "metadata": {},
   "source": [
    "자 그럼 `Run` 을 새롭게 생성하여 정답을 제출해 볼 차례입니다.\n",
    "\n",
    "`client.beta.threads.runs.submit_tool_outputs` 함수는 우리의 입력을 다시 제출할 수 있도록 해줍니다.\n",
    "\n",
    "참고\n",
    "\n",
    "- `tool_call_id`: 대기중인 `tool_call` 의 `id` 를 입력합니다.\n",
    "- `output`: 사용자가 입력할 내용을 `json.dumps` 로 json 형식으로 변환하여 제출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "acfd221c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_szuOUhiiw7IYb0smWu1PE4lj',\n",
       " 'assistant_id': 'asst_i3b9sa1FNQ8FlFXFWKuhSBin',\n",
       " 'cancelled_at': None,\n",
       " 'completed_at': None,\n",
       " 'created_at': 1707912826,\n",
       " 'expires_at': 1707913426,\n",
       " 'failed_at': None,\n",
       " 'file_ids': ['file-AwGvpuu9ESsPAHPUTIWLjeKP',\n",
       "  'file-drMUHKDG2pGSntNUGvH44E89'],\n",
       " 'instructions': 'You are an expert in generating multiple choice quizzes. Create quizzes based on uploaded files.',\n",
       " 'last_error': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo-preview',\n",
       " 'object': 'thread.run',\n",
       " 'required_action': None,\n",
       " 'started_at': 1707912827,\n",
       " 'status': 'queued',\n",
       " 'thread_id': 'thread_rwPE8rPzZimA1GdbTpSHT66s',\n",
       " 'tools': [{'type': 'retrieval'},\n",
       "  {'function': {'name': 'generate_quiz',\n",
       "    'description': \"Generate a quiz to the student, and returns the student's response. A single quiz has multiple questions.\",\n",
       "    'parameters': {'type': 'object',\n",
       "     'properties': {'title': {'type': 'string'},\n",
       "      'questions': {'type': 'array',\n",
       "       'description': 'An array of questions, each with a title and multiple choice options.',\n",
       "       'items': {'type': 'object',\n",
       "        'properties': {'question_text': {'type': 'string'},\n",
       "         'choices': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "        'required': ['question_text', 'choices']}}},\n",
       "     'required': ['title', 'questions']}},\n",
       "   'type': 'function'}],\n",
       " 'usage': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.beta.threads.runs.submit_tool_outputs(\n",
    "    thread_id=thread_id,\n",
    "    run_id=run.id,\n",
    "    tool_outputs=[\n",
    "        {\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"output\": json.dumps(responses),\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2953c96",
   "metadata": {},
   "source": [
    "아직 `Run` 만 생성했기 때문에 실제로 제출이 된 것은 아닙니다. 아래의 코드를 실행하여 제출하고 피드백을 받아보도록 합시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "818eefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "퀴즈를 제출했습니다.\n",
      "[USER]\n",
      "3개의 객관식 퀴즈(multiple choice questions)를 만들어 주세요. 객관식 퀴즈의 선택지에 번호를 표기해주세요. 1~4까지 숫자로 시작하여야 합니다. 퀴즈는 내가 업로드한 파일에 관한 내용이어야 합니다. 내가 제출한 responses에 대한 피드백을 주세요. 내가 기입한 답, 정답, 제출한 답이 오답이라면 오답에 대한 피드백을 모두 포함해야 합니다. 모든 내용은 한글로 작성해 주세요. \n",
      "\n",
      "[ASSISTANT]\n",
      "객관식 퀴즈에 대한 답변을 확인하였습니다. 여러분의 응답 및 피드백은 다음과 같습니다:\n",
      "\n",
      "1. 자연어 처리 작업에 대한 기존 접근 방식은 무엇입니까?\n",
      "   - 여러분의 응답: **2. 지도 학습을 사용한 과제별 데이터셋**\n",
      "   - 정답: **2. 지도 학습을 사용한 과제별 데이터셋**\n",
      "   - 피드백: 올바른 답변입니다! 자연어 처리 작업은 주로 지도 학습 기반의 과제별 데이터셋을 사용하는 것이 일반적입니다【8†source】.\n",
      "\n",
      "2. 바이든 대통령이 서명한 '안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령'의 핵심 내용은 무엇입니까?\n",
      "   - 여러분의 응답: **3. 생성 AI 관련 저작권 법안 제정**\n",
      "   - 정답: **1. AI의 안전과 보안 기준 마련 및 개인정보보호**\n",
      "   - 피드백: 선택하신 응답이 정답은 아닙니다. 문제의 핵심 내용은 AI의 안전과 보안 기준 마련, 개인정보보호, 형평성과 시민권 향상, 소비자 보호 등 여러 항목을 포함합니다【10†source】.\n",
      "\n",
      "3. 다음 중 AI 안전성 관련하여 영국에서 발표된 주요 내용은 무엇입니까?\n",
      "   - 여러분의 응답: **2. AI 생성 콘텐츠 표시 의무화**\n",
      "   - 정답: **3. AI 안전 연구소 설립 발표**\n",
      "   - 피드백: 선택하신 응답이 정답은 아닙니다. 영국에서 발표된 주요 내용은 AI 안전 연구소 설립에 관한 것입니다【10†source】.\n",
      "\n",
      "문제 2와 3에서 오답을 선택하신 부분에 대해 죄송하지만, 해당 문제에 대한 올바른 이해와 함께 정확한 정보 판단에 더 주의를 기울일 필요가 있을 것 같습니다. 항상 각 문제의 핵심적인 내용을 정확하게 파악하는 것이 중요합니다. 다음 기회에 더 좋은 결과가 있기를 바랍니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스레드에서 실행을 기다립니다.\n",
    "run = wait_on_run(run, thread_id)\n",
    "# 실행이 완료되면, 실행의 상태를 출력합니다.\n",
    "if run.status == \"completed\":\n",
    "    print(\"퀴즈를 제출했습니다.\")\n",
    "    # 전체 대화내용 출력\n",
    "    print_message(get_response(thread_id).data[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f3152",
   "metadata": {},
   "source": [
    "### 퀴즈 생성기 전체코드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883fc9e",
   "metadata": {},
   "source": [
    "#### 1) 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3b6cb",
   "metadata": {},
   "source": [
    "`.env` 파일로부터 API KEY를 불러옵니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY 정보를 불러옵니다\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"API KEY를 입력해 주세요\"\n",
    "# OPENAI_API_KEY 를 설정합니다.\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a55d10",
   "metadata": {},
   "source": [
    "OpenAI 객체를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API를 사용하기 위한 클라이언트 객체를 생성합니다.\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9f848",
   "metadata": {},
   "source": [
    "#### 2) 파일 업로드\n",
    "\n",
    "출제될 문제가 기반할 파일을 업로드합니다.\n",
    "\n",
    "1. 신규 파일 업로드\n",
    "2. 기존 업로드된 File ID 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e90821",
   "metadata": {},
   "source": [
    "신규 파일 업로드를 위한 코드입니다. 이미 업로드한 경우 건너 뛰어도 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 업로드를 위한 함수를 정의합니다.\n",
    "def upload_files(files):\n",
    "    uploaded_files = []\n",
    "    for filepath in files:\n",
    "        file = client.files.create(\n",
    "            file=open(\n",
    "                # 업로드할 파일의 경로를 지정합니다.\n",
    "                filepath,  # 파일경로. (예시) data/sample.pdf\n",
    "                \"rb\",\n",
    "            ),\n",
    "            purpose=\"assistants\",\n",
    "        )\n",
    "        uploaded_files.append(file)\n",
    "        print(f\"[업로드한 파일 ID]\\n{file.id}\")\n",
    "    return uploaded_files\n",
    "\n",
    "\n",
    "# 필요에 따라서는 파일을 업로드 합니다.\n",
    "# 파일 업로드시 아래 주석을 해제하고 업로드할 파일의 경로를 지정합니다.\n",
    "# 업로드할 파일들의 경로를 지정합니다.\n",
    "# files_to_upload = [\n",
    "#     \"data/language_models_are_unsupervised_multitask_learners.pdf\",\n",
    "#     \"data/SPRI_AI_Brief_2023년12월호.pdf\",\n",
    "# ]\n",
    "\n",
    "# 파일을 업로드합니다.\n",
    "# file_ids = upload_files(files_to_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd887418",
   "metadata": {},
   "source": [
    "이미 파일을 업로드 하였다면, **업로드한 파일 목록을 조회** 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b586f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업로드한 모든 파일 ID 와 파일명을 출력합니다.\n",
    "for file in client.files.list():\n",
    "    print(f\"[파일 ID] {file.id} [파일명] {file.filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant 가 참고할 파일 ID를 지정합니다.\n",
    "file_ids = [\"file-pXvkeNqOoMakeFYE9czPcHTL\", \"file-ywTSPKuBSAZD9HliWEBo1LHc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64a60d",
   "metadata": {},
   "source": [
    "#### 3) Function Schema 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f4c1f",
   "metadata": {},
   "source": [
    "다음으로는 Assistant 를 생성합니다. 생성시 tools에 function을 추가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b91301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스키마를 정의합니다.\n",
    "function_schema = {\n",
    "    \"name\": \"generate_quiz\",\n",
    "    \"description\": \"Generate a quiz to the student, and returns the student's response. A single quiz has multiple questions.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\"},\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"An array of questions, each with a title and multiple choice options.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question_text\": {\"type\": \"string\"},\n",
    "                        \"choices\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    },\n",
    "                    \"required\": [\"question_text\", \"choices\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"title\", \"questions\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 퀴즈를 출제하는 역할을 하는 챗봇을 생성합니다.\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Quiz Generator\",\n",
    "    instructions=\"You are an expert in generating multiple choice quizzes. Create quizzes based on uploaded files.\",\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    tools=[\n",
    "        {\"type\": \"retrieval\"},\n",
    "        {\"type\": \"function\", \"function\": function_schema},\n",
    "    ],\n",
    "    file_ids=file_ids,\n",
    ")\n",
    "\n",
    "ASSISTANT_ID = assistant.id\n",
    "# 생성된 챗봇의 정보를 JSON 형태로 출력합니다.\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f651ba8",
   "metadata": {},
   "source": [
    "#### 4) 퀴즈 요청\n",
    "\n",
    "- 새로운 스레드를 생성하고, Assistant 에게 상세한 지시사항과 함께 퀴즈를 출제를 요청합니다.\n",
    "- 요청에 대한 답이 오면, 퀴즈를 출제하고 사용자로부터 정답 기입을 요청합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 스레드를 생성한 뒤 진행합니다.\n",
    "thread_id = create_new_thread().id\n",
    "\n",
    "# 퀴즈를 만들도록 요청합니다.\n",
    "run = ask(\n",
    "    ASSISTANT_ID,\n",
    "    thread_id,\n",
    "    # 객관식 퀴즈에 대한 구체적인 지시사항을 기입합니다.\n",
    "    \"3개의 객관식 퀴즈(multiple choice questions)를 만들어 주세요. \"\n",
    "    \"객관식 퀴즈의 선택지에 번호를 표기해주세요. 1~4까지 숫자로 시작하여야 합니다. \"\n",
    "    \"퀴즈는 내가 업로드한 파일에 관한 내용이어야 합니다. \"\n",
    "    \"내가 제출한 responses에 대한 피드백을 주세요. \"\n",
    "    \"내가 기입한 답, 정답, 제출한 답이 오답이라면 오답에 대한 피드백을 모두 포함해야 합니다. \"\n",
    "    \"모든 내용은 한글로 작성해 주세요. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d788d",
   "metadata": {},
   "source": [
    "이제 생성된 퀴즈를 출력하고 사용자가 답변을 기입할 차례입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13165dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퀴즈를 사용자에게 표시하는 함수를 정의합니다.\n",
    "def display_quiz(title, questions, show_numeric=False):\n",
    "    print(f\"제목: {title}\\n\")\n",
    "    responses = []\n",
    "\n",
    "    for q in questions:\n",
    "        # 질문을 출력합니다.\n",
    "        print(q[\"question_text\"])\n",
    "        response = \"\"\n",
    "\n",
    "        # 각 선택지를 출력합니다.\n",
    "        for i, choice in enumerate(q[\"choices\"]):\n",
    "            if show_numeric:\n",
    "                print(f\"{i+1} {choice}\")\n",
    "            else:\n",
    "                print(f\"{choice}\")\n",
    "\n",
    "        response = input(\"정답을 선택해 주세요: \")\n",
    "        responses.append(response)\n",
    "        print()\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# requires_action 상태는 사용자의 응답 제출해야 합니다.\n",
    "# 제출이 완료될 때까지 Assistant 는 최종 답변을 대기합니다.\n",
    "# 늦게 제출시 만료(expired) 상태가 될 수 있습니다.\n",
    "if run.status == \"requires_action\":\n",
    "    # 단일 도구 호출 추출\n",
    "    tool_call = run.required_action.submit_tool_outputs.tool_calls[0]\n",
    "    name = tool_call.function.name\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    responses = display_quiz(arguments[\"title\"], arguments[\"questions\"])\n",
    "    # 퀴즈를 표시하고 사용자의 응답을 반환합니다.\n",
    "    print(\"기입한 답(순서대로)\")\n",
    "    print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ef800",
   "metadata": {},
   "source": [
    "#### 5) 피드백\n",
    "\n",
    "- 마지막으로, 퀴즈를 제출하고 피드백을 받습니다.\n",
    "- 결과를 출력하여 전체 내용을 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 답변을 제출하기 위한 Run 을 생성합니다.\n",
    "run = client.beta.threads.runs.submit_tool_outputs(\n",
    "    thread_id=thread_id,\n",
    "    run_id=run.id,\n",
    "    tool_outputs=[\n",
    "        {\n",
    "            \"tool_call_id\": tool_call.id,  # 대기중인 tool_call 의 ID\n",
    "            \"output\": json.dumps(responses),  # 사용자 답변\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 스레드에서 실행을 기다립니다.\n",
    "run = wait_on_run(run, thread_id)\n",
    "\n",
    "# 실행이 완료되면, 실행의 상태를 출력합니다.\n",
    "if run.status == \"completed\":\n",
    "    print(\"퀴즈를 제출했습니다.\")\n",
    "    # 전체 대화내용 출력\n",
    "    print_message(get_response(thread_id).data[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa36790",
   "metadata": {},
   "source": [
    "## 끝으로\n",
    "\n",
    "이번 튜토리얼은 Assistant 와 도구 3가지(`Code Interpreter`, `Retrieval`, `Functions`) 를 다뤘습니다.\n",
    "\n",
    "Assistant 를 활용한 LLM 어플리케이션 제작에 많은 도움이 되었기를 바랍니다.\n",
    "\n",
    "참고\n",
    "\n",
    "- [Annotations](https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages): 파일 인용 구문 분석\n",
    "- [Files](https://platform.openai.com/docs/api-reference/assistants/file-object): 스레드 범위 대비 어시스턴트 범위\n",
    "- [Parallel Function Calls](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling): 단일 단계에서 여러 도구를 호출하기\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
